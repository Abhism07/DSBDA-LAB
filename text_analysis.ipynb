{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea440651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ec54b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The curious cat cat jumped jumped gracefully over the fence, exploring the unknown.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"The curious cat cat jumped jumped gracefully over the fence, exploring the unknown.\"\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6af753",
   "metadata": {},
   "source": [
    "# TOKENIZING SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b95ccf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'curious', 'cat', 'cat', 'jumped', 'jumped', 'gracefully', 'over', 'the', 'fence', ',', 'exploring', 'the', 'unknown', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "token_sent = word_tokenize(sent)\n",
    "print(token_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ca9f8",
   "metadata": {},
   "source": [
    "# REMOVING PUNCTUATIONS AND STOPWORDS FROM THE TOKENIZED SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abe9447f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punc = string.punctuation\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2596a981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'curious',\n",
       " 'cat',\n",
       " 'cat',\n",
       " 'jumped',\n",
       " 'jumped',\n",
       " 'gracefully',\n",
       " 'over',\n",
       " 'the',\n",
       " 'fence',\n",
       " 'exploring',\n",
       " 'the',\n",
       " 'unknown']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sent = [c for c in token_sent if c not in punc]\n",
    "new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4456f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f95bd7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'curious',\n",
       " 'cat',\n",
       " 'cat',\n",
       " 'jumped',\n",
       " 'jumped',\n",
       " 'gracefully',\n",
       " 'fence',\n",
       " 'exploring',\n",
       " 'unknown']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sent1 = [c for c in new_sent if c not in stopwords.words(\"english\")]\n",
    "new_sent1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae4af2e",
   "metadata": {},
   "source": [
    "# PARTS OF SPEECH TAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c4bf118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('curious', 'JJ'),\n",
       " ('cat', 'NN'),\n",
       " ('cat', 'NN'),\n",
       " ('jumped', 'VBD'),\n",
       " ('jumped', 'VBD'),\n",
       " ('gracefully', 'RB'),\n",
       " ('fence', 'JJ'),\n",
       " ('exploring', 'VBG'),\n",
       " ('unknown', 'JJ')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(new_sent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c1ef99",
   "metadata": {},
   "source": [
    "# LEMMATIZATION AND STEMMING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df2407dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "curiou\n",
      "cat\n",
      "cat\n",
      "jump\n",
      "jump\n",
      "grace\n",
      "fenc\n",
      "explor\n",
      "unknown\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "for w in new_sent1:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d665d77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "curious\n",
      "cat\n",
      "cat\n",
      "jumped\n",
      "jumped\n",
      "gracefully\n",
      "fence\n",
      "exploring\n",
      "unknown\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "for w in new_sent1:\n",
    "    print(wl.lemmatize(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f4b29e",
   "metadata": {},
   "source": [
    "# TFIDF VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9e8d646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 5)\t1.0\n",
      "  (5, 5)\t1.0\n",
      "  (6, 4)\t1.0\n",
      "  (7, 3)\t1.0\n",
      "  (8, 2)\t1.0\n",
      "  (9, 7)\t1.0\n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "x=tfidf.fit_transform(new_sent1)\n",
    "print(x)\n",
    "print()\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d12ad0",
   "metadata": {},
   "source": [
    "In scikit-learn's TfidfVectorizer, the norm parameter controls the normalization of the TF-IDF matrix. Setting norm=\"none\" disables normalization, meaning that the TF-IDF values will not be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "808d93d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t2.7047480922384253\n",
      "  (1, 1)\t2.7047480922384253\n",
      "  (2, 0)\t2.2992829841302607\n",
      "  (3, 0)\t2.2992829841302607\n",
      "  (4, 5)\t2.2992829841302607\n",
      "  (5, 5)\t2.2992829841302607\n",
      "  (6, 4)\t2.7047480922384253\n",
      "  (7, 3)\t2.7047480922384253\n",
      "  (8, 2)\t2.7047480922384253\n",
      "  (9, 7)\t2.7047480922384253\n",
      "\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  2.70474809 0.        ]\n",
      " [0.         2.70474809 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [2.29928298 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [2.29928298 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         2.29928298\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         2.29928298\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         2.70474809 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         2.70474809 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         2.70474809 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         2.70474809]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(norm=None)\n",
    "x=tfidf.fit_transform(new_sent1)\n",
    "print(x)\n",
    "print()\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64264955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['the'], dtype='<U10'),\n",
       " array(['curious'], dtype='<U10'),\n",
       " array(['cat'], dtype='<U10'),\n",
       " array(['cat'], dtype='<U10'),\n",
       " array(['jumped'], dtype='<U10'),\n",
       " array(['jumped'], dtype='<U10'),\n",
       " array(['gracefully'], dtype='<U10'),\n",
       " array(['fence'], dtype='<U10'),\n",
       " array(['exploring'], dtype='<U10'),\n",
       " array(['unknown'], dtype='<U10')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.inverse_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c7f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
